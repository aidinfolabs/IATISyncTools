	#IATI Downloader
import sys
import urllib
import json
from datetime import date
import os
import hashlib
import string
import types
from urlparse import urlparse
import re

def registry_fetch(api_endpoint, *parameters): #To return an object full of all the data. Needs to handle query building and 
	query = string.join(parameters,'&')
	url = api_endpoint + 'search/dataset?' + query + '&limit=100&all_fields=1&offset='
	all_fetched = 0
	offset = 0
	dataset_list = []

	while(all_fetched == 0):
		try:
			if debug_level > 4:
				print "Fetching " + url+str(offset)
			api_connection = urllib.urlopen(url + str(offset))
			data = json.loads(api_connection.read())
		except:
			if debug_level > 0:
				print "Error fetching URL or parsing JSON for url"
		try:
			dataset_list = dataset_list + data['results']
			count = data['count']
			offset = offset + 100
			if offset > count:
				all_fetched = 1

		except TypeError:
			all_fetched = 1 # Set all fetched so we exit the process.
			if debug_level > 0:
				print "Failed loading packages with this response from the registry:" + str(data)			

	if debug_level > 3:
		print str(len(dataset_list)) + " total package details fetched"

	return dataset_list


def resource_fetch(dataset_list,directory_pattern = 'datasets/$groups',file_pattern='$name.xml',comparison_method = 'dataset_meta',store_metadata=True):
	for dataset in dataset_list:
		for resource in dataset['res_url']:
			flat_dataset = flatten_dataset_details(dataset)
			directory = string.Template(directory_pattern).substitute(flat_dataset)
			filename = string.Template(file_pattern).substitute(flat_dataset)
		
			#ToDo: Add lots of error handling and reporting
			check_dir(directory)
  		
			try:
				if check_resource(dataset,resource,directory + '/' + filename,comparison_method):
					if debug_level > 3:
						print "Fetching "+ dataset['name'] + " from " + resource + " into " + directory + "/" + filename
					webFile = urllib.urlopen(resource)
					localFile = open(directory + '/' + filename, 'w')
					localFile.write(webFile.read())
					webFile.close()
					if store_metadata:
						metaFileName, metaFileExtension = os.path.splitext(directory + '/' + filename) 
						metaFile = open(metaFileName + '.meta.json','w')
						metaFile.write(json.dumps(dataset))
						metaFile.close
						if debug_level > 4:
							print "Writing meta-data"
				else:
					if debug_level > 4:
						print "No update required for: "+ dataset['name']

			except Exception, e:
				print "Fetching "+ dataset['name'] + " from " + resource + "failed. Error: " + str(e)


# This function takes the meta-data dictionary and flattens it out, picking the first value from any list, and flatting out extras. This is primarily to make them available for templating functions.
def flatten_dataset_details(dataset):
	flattened = dict()
	for key in dataset:
		if key == 'extras':
		 	for extra_key in dataset['extras']:
		 		flattened[extra_key] = dataset['extras'][extra_key]
		elif type(dataset[key]) == types.ListType:
			flattened[key] = str(dataset[key][0])
		else:
			flattened[key] = dataset[key]
	return flattened
	

def check_dir(dir):
    if not os.path.exists(dir):
        try:
            os.makedirs(dir)
        except Exception, e:
            print "Directory creation failed:", e
            print "Couldn't create directory " + dir

# Check resource will determine whether or not the required resources has been updated since last downloaded
#
# It offers a number of comparaison methods
# 1) dataset_meta - compares the last_modified date in the dataset meta-data with the last_modified data in the cached meta-data.
#   This date is generated by the registry reading the most recent last-updated-datetime value for all the activities in a file on it's regular check of data files.
#
# 2) resource_meta - compares the last time the registry noticed a change in the actual file during it's regular check of data files with the last modified time of the file on disk. 
#
# 3) file_hash - compares the sha1 hash the registry calculates over files with the sha1 hash of the file on disk
# 
# 4) host_header - compares the last_modified time reported by the web server where the file is hosted with the last modified time of the file on disk
#
# (1) uses data we already have to hand, trusting the publisher to update last-updated-datetime correctly; 
# (2) and (3) require an additional request to the registry API
# (4) requires an additional request to the hosting web server
#
def check_resource(dataset,url,filepath,method = ''):
	# First check if file exists
	try:
		open(filepath)
	except: 
		if debug_level > 3:
			print "Stored file not found."
		return True

	if method == 'dataset_meta':
		metaFileName, metaFileExtension = os.path.splitext(filepath) 
		try:
			metaFile = open(metaFileName + '.meta.json','r')
			metadata = json.loads(metaFile.read())
			if dataset['extras']['data_updated'] and (dataset['extras']['data_updated'] == metadata['extras']['data_updated']):
				if debug_level > 4:
					print "Dataset data_updated values match. No update required."
				return False
			else:
				if debug_level > 4:
					print "Dataset data_updated values differ. Update required."
				return True
		except:
			if debug_level > 3:
				print "Stored metadata file not found."
			return True
	elif method == 'host_header':
		print "Host header comparison not yet implemented"
		pass
	elif method == 'file_hash':
		url_parts = urlparse(dataset['ckan_url'])
		url = 'http://' + url_parts[1] + '/api/rest/package/' + dataset['name']
		try:
			api_connection = urllib.urlopen(url)
			data = json.loads(api_connection.read())
			if data['resources'][0]['hash']:
				try:
					checkfile = open(filepath)
					if unicode(hashlib.sha1(re.sub('generated-datetime="(.*)"','',checkfile.read())).hexdigest()) == data['resources'][0]['hash']:
						if debug_level > 3:
							print "Hashes match for " + dataset['name'] + " - No update required."
						return False
					else:
						if debug_level > 3:
							print "Hashes do not match for " + dataset['name'] + " - Update required."		
						return True 
				except:
					print "Stored file not found."
					return True
			else:
				if debug_level > 4:
					print "No hash available. Update required."
				return True
		except:
			if debug_level > 2:
				print "Failed to get metadata from registry for " + dataset['name']
		
		return False

	elif method == 'resource_meta':
		print "The resource_meta comparison is not yet implemented."
		return True
	else: #We've not been passed a method for comparison, so require an update. 
		return True 
	

if __name__ == '__main__':
	global debug_level
	debug_level = 3 # 0 = silent | 1 = error | 3 = notices | 5 = verbose

	import argparse
	parser = argparse.ArgumentParser(description='Download data packages from the IATI Registry')
	parser.add_argument('--filetype', dest='parameter_filetype', type=str, nargs='?', help='The type of file to search for: activity or organisations.')
	parser.add_argument('--publisher', dest='parameter_group', type=str, nargs='?', help='Only fetch files for a particular publisher.')
	parser.add_argument('--country', dest='parameter_country', type=str, nargs='?', help='Only fetch files for a given country or region codes to fetch data for')
	parser.add_argument('--search', dest='search', type=str, nargs='*', help='Any search parameters supported for datasets by the CKAN API (see http://docs.ckan.org/en/latest/api-v2.html). In the format parameter=value. E.g verified=yes')
	parser.add_argument('-m','--store_metadata', dest='metadata', help='Flag to store metadata alongside downloads. Defaults to on', choices=['on','off'],type=str)
	parser.add_argument('-d','--directory_pattern', dest='directory_pattern', type=str, nargs='?', help='Path to save files in. Can use replacement patterns for $name, $group, $license_id etc.')
	parser.add_argument('-f','--file_pattern', dest='file_pattern', type=str, nargs='?', help='Filename to use. Can use replacement patterns for $name, $group, $license_id etc.')
	parser.add_argument('-c','--comparison_method', dest='comparison_method', type=str, nargs='?', help='', choices=['dataset_meta','resource_meta','file_hash','host_header'])
	parser.add_argument('-v','--verbosity', dest='verbosity', type=int, nargs=1, help='', choices=range(1,6))


# Check resource will determine whether or not the required resources has been updated since last downloaded
#
# It offers a number of comparaison methods
# 1) dataset_meta - compares the last_modified date in the dataset meta-data with the last_modified data in the cached meta-data.
#   This date is generated by the registry reading the most recent last-updated-datetime value for all the activities in a file on it's regular check of data files.
#
# 2) resource_meta - compares the last time the registry noticed a change in the actual file during it's regular check of data files with the last modified time of the file on disk. 
#
# 3) file_hash - compares the sha1 hash the registry calculates over files with the sha1 hash of the file on disk
# 
# 4) host_header - compares the last_modified time reported by the web server where the file is hosted with the last modified time of the file on disk
#
# (1) uses data we already have to hand, trusting the publisher to update last-updated-datetime correctly; 
# (2) and (3) require an additional request to the registry API

	args = parser.parse_args()

	if(args.parameter_filetype):
		filetype=args.parameter_filetype
	else:
		filetype=''
	if(args.parameter_group):
		groups = 'groups='+args.parameter_group
	else:
		groups = ''
	if(args.parameter_country):
		country = 'country='+args.parameter_country
	else:
		country = ''
	if(args.search):
		search = string.join(args.search,'&')
	else:
		search = ''
	if(args.metadata) == 'off':
		metadata = False
	else:
		metadata = True
	if(args.directory_pattern):
		directory_pattern = args.directory_pattern
	else:
		directory_pattern = 'datasets/$groups'
	if(args.file_pattern):
		file_pattern = args.file_pattern
	else:
		file_pattern = '$name.xml'
	if(args.comparison_method):
		comparison = args.comparison_method
	else:
		comparison = ''
	if(args.verbosity):
		debug_level = args.verbosity
	else:
		verbosity = 3


	dataset_list = registry_fetch('http://www.iatiregistry.org/api/',filetype,search,groups,country)

	resource_fetch(dataset_list,directory_pattern,file_pattern,comparison,metadata)
    
#    
#    import sys
#    dir="~/iati/data/packages/"
#    dir = os.path.expanduser(dir)
#    check_dir(dir)
#    run(dir,args.groups)
